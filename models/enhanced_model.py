#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
- Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (—Ü–µ–Ω–∞ + –Ω–æ–≤–æ—Å—Ç–∏ + –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)
- Attention –º–µ—Ö–∞–Ω–∏–∑–º—ã
- Ensemble –º–µ—Ç–æ–¥—ã
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from typing import Optional, Tuple, List, Dict, Any
import numpy as np
import pandas as pd
from dataclasses import dataclass

@dataclass
class ModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
    # –†–∞–∑–º–µ—Ä—ã –≤—Ö–æ–¥–æ–≤
    price_features: int = 8
    news_features: int = 64
    indicator_features: int = 26
    
    # Transformer –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    d_model: int = 256
    nhead: int = 8
    num_layers: int = 6
    dim_feedforward: int = 1024
    dropout: float = 0.1
    max_seq_length: int = 100
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    num_classes: int = 3  # DOWN, NEUTRAL, UP
    use_news: bool = True
    use_indicators: bool = True
    
    # –û–±—É—á–µ–Ω–∏–µ
    learning_rate: float = 1e-4
    weight_decay: float = 1e-5
    warmup_steps: int = 1000

class PositionalEncoding(nn.Module):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Transformer"""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: Tensor) -> Tensor:
        return x + self.pe[:x.size(0), :]

class MultiHeadAttention(nn.Module):
    """–ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % nhead == 0
        
        self.d_model = d_model
        self.nhead = nhead
        self.d_k = d_model // nhead
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = torch.sqrt(torch.FloatTensor([self.d_k]))
    
    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
        batch_size = query.shape[0]
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        Q = self.w_q(query).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)
        
        # Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale.to(query.device)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention = F.softmax(scores, dim=-1)
        attention = self.dropout(attention)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–Ω–∏–º–∞–Ω–∏–µ
        out = torch.matmul(attention, V)
        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        out = self.w_o(out)
        
        return out, attention

class TransformerBlock(nn.Module):
    """–ë–ª–æ–∫ Transformer —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self, d_model: int, nhead: int, dim_feedforward: int, dropout: float = 0.1):
        super().__init__()
        
        self.attention = MultiHeadAttention(d_model, nhead, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.feedforward = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward, d_model),
            nn.Dropout(dropout)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
        # Self-attention
        attn_out, attention = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_out))
        
        # Feedforward
        ff_out = self.feedforward(x)
        x = self.norm2(x + ff_out)
        
        return x, attention

class FeatureEncoder(nn.Module):
    """–ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.price_encoder = nn.Sequential(
            nn.Linear(config.price_features, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, config.d_model // 2)
        )
        
        # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        if config.use_news:
            self.news_encoder = nn.Sequential(
                nn.Linear(config.news_features, 128),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(128, config.d_model // 2)
            )
        
        # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        if config.use_indicators:
            self.indicator_encoder = nn.Sequential(
                nn.Linear(config.indicator_features, 128),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(128, config.d_model // 2)
            )
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ –æ–±—â–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_projection = nn.Linear(config.d_model, config.d_model)
    
    def forward(self, price_data: Tensor, news_data: Optional[Tensor] = None, 
                indicators: Optional[Tensor] = None) -> Tensor:
        batch_size = price_data.shape[0]
        
        # –ö–æ–¥–∏—Ä—É–µ–º —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        price_encoded = self.price_encoder(price_data)
        
        # –ö–æ–¥–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏
        if self.config.use_news and news_data is not None:
            news_encoded = self.news_encoder(news_data)
        else:
            news_encoded = torch.zeros(batch_size, self.config.d_model // 2, device=price_data.device)
        
        # –ö–æ–¥–∏—Ä—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        if self.config.use_indicators and indicators is not None:
            indicator_encoded = self.indicator_encoder(indicators)
        else:
            indicator_encoded = torch.zeros(batch_size, self.config.d_model // 2, device=price_data.device)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        combined_features = torch.cat([price_encoded, news_encoded, indicator_encoded], dim=-1)
        
        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –≤ –æ–±—â–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
        encoded_features = self.feature_projection(combined_features)
        
        return encoded_features

class CryptoTransformer(nn.Module):
    """–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.feature_encoder = FeatureEncoder(config)
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.pos_encoding = PositionalEncoding(config.d_model, config.max_seq_length)
        
        # Transformer –±–ª–æ–∫–∏
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(config.d_model, config.nhead, config.dim_feedforward, config.dropout)
            for _ in range(config.num_layers)
        ])
        
        # –í—ã—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–∏
        self.output_projection = nn.Linear(config.d_model, config.d_model)
        self.classifier = nn.Linear(config.d_model, config.num_classes)
        
        # Dropout
        self.dropout = nn.Dropout(config.dropout)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._init_weights()
    
    def _init_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def create_padding_mask(self, seq_lengths: List[int], max_len: int) -> Tensor:
        """–°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è padding"""
        batch_size = len(seq_lengths)
        mask = torch.zeros(batch_size, max_len, dtype=torch.bool)
        
        for i, length in enumerate(seq_lengths):
            mask[i, length:] = True
        
        return mask
    
    def forward(self, price_data: Tensor, news_data: Optional[Tensor] = None,
                indicators: Optional[Tensor] = None, seq_lengths: Optional[List[int]] = None) -> Dict[str, Tensor]:
        batch_size, seq_len, _ = price_data.shape
        
        # –ö–æ–¥–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        encoded_features = self.feature_encoder(price_data, news_data, indicators)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        encoded_features = encoded_features.transpose(0, 1)  # (seq_len, batch, d_model)
        encoded_features = self.pos_encoding(encoded_features)
        encoded_features = encoded_features.transpose(0, 1)  # (batch, seq_len, d_model)
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è padding
        if seq_lengths is not None:
            padding_mask = self.create_padding_mask(seq_lengths, seq_len)
        else:
            padding_mask = None
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ Transformer –±–ª–æ–∫–∏
        attention_weights = []
        x = encoded_features
        
        for transformer_block in self.transformer_blocks:
            x, attention = transformer_block(x, padding_mask)
            attention_weights.append(attention)
        
        # –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if padding_mask is not None:
            # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏
            mask_expanded = padding_mask.unsqueeze(-1).expand_as(x)
            x_masked = x.masked_fill(mask_expanded, 0.0)
            seq_lengths_tensor = torch.tensor(seq_lengths, dtype=torch.float, device=x.device)
            x_pooled = x_masked.sum(dim=1) / seq_lengths_tensor.unsqueeze(-1)
        else:
            x_pooled = x.mean(dim=1)
        
        # –í—ã—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–∏
        x_projected = self.output_projection(x_pooled)
        x_projected = self.dropout(x_projected)
        
        logits = self.classifier(x_projected)
        probs = F.softmax(logits, dim=-1)
        
        return {
            "logits": logits,
            "probs": probs,
            "attention_weights": attention_weights,
            "encoded_features": encoded_features
        }

class EnsembleModel(nn.Module):
    """Ensemble –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ö–æ–¥–æ–≤"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        # –û—Å–Ω–æ–≤–Ω–∞—è Transformer –º–æ–¥–µ–ª—å
        self.transformer_model = CryptoTransformer(config)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è ensemble
        self.lstm_model = nn.LSTM(
            input_size=config.d_model,
            hidden_size=128,
            num_layers=2,
            dropout=0.1,
            batch_first=True
        )
        
        self.gru_model = nn.GRU(
            input_size=config.d_model,
            hidden_size=128,
            num_layers=2,
            dropout=0.1,
            batch_first=True
        )
        
        # Ensemble –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.ensemble_classifier = nn.Sequential(
            nn.Linear(config.num_classes * 3, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, config.num_classes)
        )
    
    def forward(self, price_data: Tensor, news_data: Optional[Tensor] = None,
                indicators: Optional[Tensor] = None, seq_lengths: Optional[List[int]] = None) -> Dict[str, Tensor]:
        
        # Transformer –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        transformer_output = self.transformer_model(price_data, news_data, indicators, seq_lengths)
        transformer_probs = transformer_output["probs"]
        
        # LSTM –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        encoded_features = transformer_output["encoded_features"]
        lstm_out, _ = self.lstm_model(encoded_features)
        lstm_pooled = lstm_out.mean(dim=1)
        lstm_logits = nn.Linear(lstm_pooled.shape[-1], self.config.num_classes)(lstm_pooled)
        lstm_probs = F.softmax(lstm_logits, dim=-1)
        
        # GRU –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        gru_out, _ = self.gru_model(encoded_features)
        gru_pooled = gru_out.mean(dim=1)
        gru_logits = nn.Linear(gru_pooled.shape[-1], self.config.num_classes)(gru_pooled)
        gru_probs = F.softmax(gru_logits, dim=-1)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        combined_probs = torch.cat([transformer_probs, lstm_probs, gru_probs], dim=-1)
        ensemble_logits = self.ensemble_classifier(combined_probs)
        ensemble_probs = F.softmax(ensemble_logits, dim=-1)
        
        return {
            "transformer_probs": transformer_probs,
            "lstm_probs": lstm_probs,
            "gru_probs": gru_probs,
            "ensemble_probs": ensemble_probs,
            "ensemble_logits": ensemble_logits,
            "attention_weights": transformer_output["attention_weights"]
        }

class CryptoAnalyzer:
    """–í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"""
    
    def __init__(self, config: ModelConfig, model_path: Optional[str] = None):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        self.model = EnsembleModel(config).to(self.device)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
        if model_path and torch.cuda.is_available():
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        
        self.model.eval()
    
    def preprocess_data(self, price_data: np.ndarray, news_data: Optional[np.ndarray] = None,
                       indicators: Optional[np.ndarray] = None) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏"""
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        price_tensor = torch.FloatTensor(price_data).to(self.device)
        
        news_tensor = None
        if news_data is not None and self.config.use_news:
            news_tensor = torch.FloatTensor(news_data).to(self.device)
        
        indicators_tensor = None
        if indicators is not None and self.config.use_indicators:
            indicators_tensor = torch.FloatTensor(indicators).to(self.device)
        
        return price_tensor, news_tensor, indicators_tensor
    
    def predict(self, price_data: np.ndarray, news_data: Optional[np.ndarray] = None,
                indicators: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """–î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        
        with torch.no_grad():
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            price_tensor, news_tensor, indicators_tensor = self.preprocess_data(
                price_data, news_data, indicators
            )
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            output = self.model(price_tensor, news_tensor, indicators_tensor)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            ensemble_probs = output["ensemble_probs"].cpu().numpy()[0]
            transformer_probs = output["transformer_probs"].cpu().numpy()[0]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å
            predicted_class = np.argmax(ensemble_probs)
            confidence = ensemble_probs[predicted_class]
            
            # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
            class_mapping = {0: "DOWN", 1: "NEUTRAL", 2: "UP"}
            prediction = class_mapping[predicted_class]
            
            return {
                "prediction": prediction,
                "confidence": float(confidence),
                "probabilities": {
                    "down": float(ensemble_probs[0]),
                    "neutral": float(ensemble_probs[1]),
                    "up": float(ensemble_probs[2])
                },
                "transformer_probs": {
                    "down": float(transformer_probs[0]),
                    "neutral": float(transformer_probs[1]),
                    "up": float(transformer_probs[2])
                },
                "attention_weights": [w.cpu().numpy() for w in output["attention_weights"]]
            }
    
    def analyze_market(self, price_data: np.ndarray, news_data: Optional[np.ndarray] = None,
                      indicators: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞"""
        
        # –ë–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction_result = self.predict(price_data, news_data, indicators)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        analysis = {
            "prediction": prediction_result,
            "market_analysis": {
                "trend_strength": self._calculate_trend_strength(price_data),
                "volatility": self._calculate_volatility(price_data),
                "support_resistance": self._find_support_resistance(price_data),
                "momentum": self._calculate_momentum(price_data)
            },
            "risk_assessment": {
                "risk_level": self._assess_risk(prediction_result, price_data),
                "confidence_interval": self._calculate_confidence_interval(prediction_result),
                "recommendation": self._generate_recommendation(prediction_result)
            }
        }
        
        return analysis
    
    def _calculate_trend_strength(self, price_data: np.ndarray) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∏–ª—É —Ç—Ä–µ–Ω–¥–∞"""
        if len(price_data) < 20:
            return 0.0
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Ç–æ—á–µ–∫ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ç—Ä–µ–Ω–¥–∞
        recent_prices = price_data[-20:, 3]  # Close prices
        
        # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
        x = np.arange(len(recent_prices))
        slope, _ = np.polyfit(x, recent_prices, 1)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∫–ª–æ–Ω
        price_range = recent_prices.max() - recent_prices.min()
        if price_range == 0:
            return 0.0
        
        trend_strength = slope / price_range * 100
        return np.clip(trend_strength, -1.0, 1.0)
    
    def _calculate_volatility(self, price_data: np.ndarray) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å"""
        if len(price_data) < 20:
            return 0.0
        
        returns = np.diff(price_data[:, 3]) / price_data[:-1, 3]  # Returns
        volatility = np.std(returns) * np.sqrt(252)  # Annualized volatility
        return float(volatility)
    
    def _find_support_resistance(self, price_data: np.ndarray) -> Dict[str, float]:
        """–ù–∞—Ö–æ–¥–∏–º —É—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è"""
        if len(price_data) < 50:
            return {"support": 0.0, "resistance": 0.0}
        
        recent_prices = price_data[-50:, 3]  # Close prices
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ —É—Ä–æ–≤–Ω–µ–π
        support = np.percentile(recent_prices, 25)
        resistance = np.percentile(recent_prices, 75)
        
        return {
            "support": float(support),
            "resistance": float(resistance)
        }
    
    def _calculate_momentum(self, price_data: np.ndarray) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–æ–º–µ–Ω—Ç"""
        if len(price_data) < 14:
            return 0.0
        
        # RSI-like momentum
        recent_prices = price_data[-14:, 3]  # Close prices
        gains = np.maximum(np.diff(recent_prices), 0)
        losses = np.maximum(-np.diff(recent_prices), 0)
        
        avg_gain = np.mean(gains)
        avg_loss = np.mean(losses)
        
        if avg_loss == 0:
            return 1.0
        
        rs = avg_gain / avg_loss
        momentum = 100 - (100 / (1 + rs))
        
        return float(momentum)
    
    def _assess_risk(self, prediction_result: Dict, price_data: np.ndarray) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞"""
        confidence = prediction_result["confidence"]
        volatility = self._calculate_volatility(price_data)
        
        if confidence < 0.6 or volatility > 0.8:
            return "HIGH"
        elif confidence < 0.8 or volatility > 0.5:
            return "MEDIUM"
        else:
            return "LOW"
    
    def _calculate_confidence_interval(self, prediction_result: Dict) -> Dict[str, float]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª"""
        confidence = prediction_result["confidence"]
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ñ–æ—Ä–º—É–ª–∞ –¥–ª—è –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
        margin = (1 - confidence) * 0.5
        
        return {
            "lower": max(0.0, confidence - margin),
            "upper": min(1.0, confidence + margin)
        }
    
    def _generate_recommendation(self, prediction_result: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é"""
        prediction = prediction_result["prediction"]
        confidence = prediction_result["confidence"]
        
        if confidence < 0.6:
            return "WAIT - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏"
        
        if prediction == "UP":
            return "BUY - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–∫—É–ø–∫–∞"
        elif prediction == "DOWN":
            return "SELL - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–¥–∞–∂–∞"
        else:
            return "HOLD - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–µ—Ä–∂–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
def create_enhanced_model(config: ModelConfig) -> CryptoAnalyzer:
    """–°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"""
    return CryptoAnalyzer(config)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    config = ModelConfig(
        price_features=8,
        news_features=64,
        indicator_features=26,
        d_model=256,
        nhead=8,
        num_layers=6,
        use_news=True,
        use_indicators=True
    )
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    analyzer = create_enhanced_model(config)
    
    print("üöÄ –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç —Å–æ–∑–¥–∞–Ω–∞!")
    print(f"üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config}")
    print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {analyzer.device}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_price_data = np.random.randn(100, 8)  # 100 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫, 8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    test_news_data = np.random.randn(100, 64)  # 100 –Ω–æ–≤–æ—Å—Ç–µ–π, 64 –ø—Ä–∏–∑–Ω–∞–∫–∞
    test_indicators = np.random.randn(100, 26)  # 100 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, 26 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    print("\nüîÆ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...")
    result = analyzer.predict(test_price_data, test_news_data, test_indicators)
    
    print(f"üìà –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result['prediction']}")
    print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.3f}")
    print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {result['probabilities']}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    print("\nüìä –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑...")
    analysis = analyzer.analyze_market(test_price_data, test_news_data, test_indicators)
    
    print(f"üìà –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞: {analysis['market_analysis']['trend_strength']:.3f}")
    print(f"üìä –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {analysis['market_analysis']['volatility']:.3f}")
    print(f"‚ö†Ô∏è –£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {analysis['risk_assessment']['risk_level']}")
    print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {analysis['risk_assessment']['recommendation']}")
